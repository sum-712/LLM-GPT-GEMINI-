# LLM-GPT-GEMINI 프로젝트

## 2026학년도 수능 국어 문항을 기반으로 GPT와 Gemini의 문제 해결 성능을 비교·분석한 연구 프로젝트

본 프로젝트는 **AI 활용 확대**에 따라, **GPT**(OpenAI)와 **Gemini**(Google)의 2026학년도 대학수학능력시험 국어 영역 문제 해결 능력을 **정량적** 및 **정성적**으로 비교·분석하는 연구입니다.  
단순히 **정답률**을 비교하는 것에 그치지 않고, 세 가지 프롬프트 전략(A/B/C)에 따른 모델 성능 변화를 체계적으로 검증하였습니다.

---

## 1. 프로젝트 개요

### 1.1 목적
- **GPT와 Gemini**의 문제 해결 능력을 동일한 조건에서 비교하여, 두 모델 간 차이를 파악합니다.
- 프롬프트 설계 방식(A/B/C)에 따라 모델 성능의 변화, 즉 프롬프트 민감도(prompt sensitivity)를 정량적으로 분석합니다.
- LLM 기반의 **학습 및 교육 프로그램** 설계를 위한 최적 프롬프트 전략을 제시합니다.

### 1.2 테스트 데이터
- **2026학년도 대학수학능력시험** 국어 영역
- **독서/문학 혼합형 34문항**(공통 문항)

### 1.3 프롬프트 전략

| 전략  | 명칭      | 핵심 지시                           | 특징                            |
|-------|-----------|-------------------------------------|---------------------------------|
| A     | 단순 지시  | “웹 검색 없이 정답만 나열하라.”      | CoT 금지, 최소 정보 제공       |
| B     | 역할 부여  | “고3 수험생 페르소나” + 정답만 요구  | 단순, 최적 모드 유도, 일관성 높음|
| C     | CoT 유도   | 출제 의도 → 근거 확인 → 지문 분석 → 정답 도출 (4단계 CoT) | 구조화된 사고 과정 강제       |

### 1.4 데이터 출처
본 연구는 한국교육과정평가원(KICE)에서 공식 제공한  
2026학년도 대학수학능력시험 국어 영역 공개문항 PDF 원본을 기반으로 분석을 수행하였습니다.  
문항 구성과 정답 정보는 PDF 원본과 동일하게 유지하여 분석의 신뢰성을 확보하였습니다.

---

## 2. 실험 결과 및 분석

### 2.1 모델별 정답률 비교

| 모델       | A(단순 지시)  | B(역할 부여)  | C(CoT 유도)   |
|------------|---------------|---------------|---------------|
| **Gemini Pro** | 18개 (52.9%)  | **28개 (82.4%)**  | 26개 (76.5%)  |
| **GPT Pro**    | 5개 (14.7%)   | **24개 (70.6%)**  | 14개 (41.2%)  |

- **Gemini Pro**는 **B(역할 부여)** 전략에서 가장 높은 성능을 보였고, 프롬프트 형식 변화에 대한 변동이 적고 안정적이었습니다.
- **GPT Pro**는 **B 전략**에서 성능이 급증했으며, C(CoT 강제)에서는 오히려 성능이 하락했습니다.

### 2.2 주요 해석

- **Gemini Pro**: **B 전략**에서 성능이 뛰어나며, **역할 부여**가 모델의 추론 및 정답률에 중요한 영향을 미쳤습니다.
- **GPT Pro**: **프롬프트 민감도**가 매우 커서 **B 전략**에서 성능이 급증했지만, **C 전략**에서는 성능 저하가 있었습니다.

---

## 3. 대학생 학습 활용 분석

### 3.1 학습 목적 및 프롬프트 전략

| 상황 | 학습 목적         | 프롬프트 전략          | 주요 평가 기준 |
|------|------------------|------------------------|----------------|
| A    | 논문 요약         | 역할 부여 + 표 형식 + 키워드 제약 | 정보 압축력, 형식 준수 |
| B    | 반박 토론 구성    | CoT 유도 + 반대 논리 설계 | 단계별 논리 전개, 추론 능력 |
| C    | 어려운 용어 비유  | 역할 부여 + 실생활 연결 | 창의성, 문맥 적응력 |

### 3.2 GPT vs. Gemini 성능 비교

| 항목      | Gemini Pro  | GPT Pro        |
|-----------|-------------|----------------|
| **구조 이행**   | 정확 (표, 키워드 제한 등 준수) | 유연한 해석, 형식 일탈 가능 |
| **논리 전개**   | 단계적 구성, CoT 명확 | 풍부한 내용, 논리 흐름 추적 어려움 |
| **비유 생성**   | 실용적, 목적 중심 | 참신하고 도전적인 연결 시도 |
| **적합 영역**   | 실제 과제 수행, 발표용 자료 | 아이디어 도출, 창의적 브레인스토밍 |

---

## 4. 종합 시사점 및 활용 가이드

### 4.1 핵심 통합 인사이트

| 항목      | 시사점 |
|-----------|--------|
| **수능 정답률 분석** | **페르소나 + 단일 목적 지시**가 모델의 문제 풀이 능력을 가장 안정적으로 끌어냄. |
| **대학생 학습 활용** | 학습 목적에 따라 프롬프트 전략을 다르게 설계할 때 LLM의 잠재력이 극대화됨. |
| **공통 전략** | **역할 부여**가 사고 흐름을 고정시키며, **지시 사항은 간결하고 명확하게 유지**하는 것이 중요. |

### 4.2 학습 상황별 LLM 추천 가이드

| 학습 목적             | 추천 LLM      | 이유                          |
|----------------------|---------------|-------------------------------|
| **지식 요약 / 시험 대비** | **Gemini Pro** | 정해진 형식과 지시 이행에 강함 |
| **논술 / 토론 연습**    | **Gemini Pro** | 단계적 추론, 논리적 전개 우수 |
| **발표 자료 / 비유 설명** | **Gemini Pro** | 실용적 표현, 목적 적합성 뛰어남 |
| **아이디어 브레인스토밍** | **GPT Pro**    | 자유로운 창의성, 참신한 연결 강점 |

### 4.3 기술적 관점에서의 모델 성능 해석

기술적 관점에서 볼 때 두 모델 간 절대적인 우위는 존재하지 않습니다.  
GPT Pro는 유연성과 창의적 발산에서 강점을 보이며,  
Gemini Pro는 구조적 정합성과 논리적 분절 능력에서 우수한 결과를 보입니다.

본 프로젝트의 실험 항목은 표 구성, 단계별 추론, 구조적 정리 등  
형식적 명확성을 요구하는 과제였기 때문에 Gemini Pro가 상대적으로 높은 점수를 기록했습니다.  

반대로 자유 형식의 에세이, 비정형적 글쓰기, 창의적 서술이 중심인 과제에서는  
GPT Pro가 더 높은 평가를 받을 가능성이 큽니다.

따라서 LLM의 활용은 우열이 아니라  
과제의 성격(정형 vs 비정형)에 따라 전략적으로 모델을 선택하는 방식이 가장 합리적입니다.

---

## 5. 결론

이 프로젝트는 **LLM의 활용 성능이 단순히 모델의 성능에만 의존하는 것이 아니라, 프롬프트 전략에 따라 크게 달라진다**는 점을 실증적으로 보여주었습니다.

- **수능 문제풀이**에는 **페르소나 기반 + 단일 목적 지시**가 가장 효과적입니다.
- **대학생 과제**에는 **상황별 전략화된 프롬프트**가 중요합니다.
- **GPT와 Gemini**는 단순 비교가 아닌 **목적에 따른 선택**이 핵심임을 알 수 있었습니다.


---

## ⭐ Support
If this project was useful to you, a Star(★) would be greatly appreciated.
프로젝트가 도움이 되었다면 Star(★)로 응원해주세요.
